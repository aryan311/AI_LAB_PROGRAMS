# -*- coding: utf-8 -*-
"""AI_Week5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1loIaYhpO4XO-FyKq44kLOUBzh05JmcX8

Aryan Kumar
"""

!pip install tf-agents

import abc
import numpy as np
import tensorflow as tf

from tf_agents.agents import tf_agent
from tf_agents.drivers import driver
from tf_agents.environments import py_environment
from tf_agents.environments import tf_environment
from tf_agents.environments import tf_py_environment
from tf_agents.policies import tf_policy
from tf_agents.specs import array_spec
from tf_agents.specs import tensor_spec
from tf_agents.trajectories import time_step as ts
from tf_agents.trajectories import trajectory
from tf_agents.trajectories import policy_step

nest = tf.nest

"""Exercise 1 -Create a environment

a. Observation between 5 and -5, Actions in (0,1,2)
For which the observation is a random integer between -5 and 5, there are 3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.
"""

class MultiArmedBanditEnv(py_environment.PyEnvironment):
  def __init__(self):
    self._observation_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')
    self._action_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')
    self._episode_ended = False
    self._observation = None
    self._reward = None

  def action_spec(self):
    return self._action_spec

  def observation_spec(self):
    return self._observation_spec

  def _reset(self):
    self._episode_ended = False
    self._observation = np.random.randint(low=-5, high=6)
    self._reward = 0
    return ts.restart(np.array(self._observation, dtype=np.int32))

  def _step(self, action):
    if self._episode_ended:
      return self.reset()

    self._reward = self._observation * action
    self._episode_ended = True
    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)

"""b. Actions on Observation Sign (0 is -ve, 2 if +ve)


Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.


"""

def optimal_policy(observation):
  if observation < 0:
    return 0
  else:
    return 2

"""c. Reward for 50 Observations


Request for 50 observations from the environment, compute and print the total reward.
"""

env = MultiArmedBanditEnv()
total_reward = 0

for _ in range(50):
  time_step = env.reset()
  action = optimal_policy(time_step.observation)
  time_step = env.step(action)
  total_reward += time_step.reward

print('Total reward:', total_reward)

"""Exercise 2 â€“Create an environment

a. Two types of reward

Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.
"""

class RewardEnv(py_environment.PyEnvironment):
  def __init__(self, reward_sign):
    self._observation_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')
    self._action_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')
    self._episode_ended = False
    self._observation = None
    self._reward = None
    self._reward_sign = reward_sign

  def action_spec(self):
    return self._action_spec

  def observation_spec(self):
    return self._observation_spec

  def _reset(self):
    self._episode_ended = False
    self._observation = np.random.randint(low=-5, high=6)
    self._reward = 0
    return ts.restart(np.array(self._observation, dtype=np.int32))

  def _step(self, action):
    if self._episode_ended:
      return self.reset()

    if self._reward_sign == 'original':
      self._reward = self._observation * action
    else:
      self._reward = -self._observation * action

    self._episode_ended = True
    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)

"""b. Different Cases of Policy

Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:

i.The agent has not detected know yet which version of the environment is running.

ii.The agent detected that the original version of the environment is running.


iii.The agent detected that the flipped version of the environment is running.
"""

class Policy:
  def __init__(self):
    self._state = 'unknown'

  def get_action(self, observation):
    if self._state == 'unknown':
      if observation >= 0:
        self._state = 'original'
        return 2
      else:
        self._state = 'flipped'
        return 0
    elif self._state == 'original':
      return 2
    else:
      return 0

"""c. Agent Detecting sign

Define the agent that detects the sign of the environment and sets the policy appropriately.
"""

class Agent:
  def __init__(self):
    self._policy = Policy()

  def update_policy(self, reward_sign):
    if reward_sign == 'original':
      self._policy._state = 'original'
    else:
      self._policy._state = 'flipped'

  def get_action(self, observation):
    return self._policy.get_action(observation)

"""Here's an example output for each case:

Case 1: Original Environment (reward = observation * action)
"""

reward_env = RewardEnv(reward_sign='original')
agent = Agent()

total_reward = 0
for i in range(50):
    observation = reward_env.reset().observation
    agent.update_policy('original')
    action = agent.get_action(observation)
    time_step = reward_env.step(action)
    total_reward += time_step.reward
print("Total reward: ", total_reward)

"""The total reward should be positive since the optimal policy will result in positive rewards for positive observations and negative rewards for negative observations

Case 2: Flipped Environment (reward = -observation * action)
"""

reward_env = RewardEnv(reward_sign='flipped')
agent = Agent()

total_reward = 0
for i in range(50):
    observation = reward_env.reset().observation
    agent.update_policy('flipped')
    action = agent.get_action(observation)
    time_step = reward_env.step(action)
    total_reward += time_step.reward
print("Total reward: ", total_reward)

"""This defines an environment where the reward is always 0, regardless of the action and observation. Therefore, the total reward in this environment should always be 0.

"""